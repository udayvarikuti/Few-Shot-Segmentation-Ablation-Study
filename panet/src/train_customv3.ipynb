{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.optim.lr_scheduler import MultiStepLR\n",
    "from torchvision.transforms import Compose\n",
    "from models.fewshotv3 import FewShotSegV3\n",
    "from models.fewshot import FewShotSeg\n",
    "# from models.fewshotv3Inet import FewShotSegV3Inet\n",
    "# import segmentation_models_pytorch as smp\n",
    "from dataloaders.customized import voc_fewshot, coco_fewshot\n",
    "from dataloaders.transforms import RandomMirror, Resize, ToTensorNormalize\n",
    "from util.utils import set_seed, CLASS_LABELS\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "import numpy as np\n",
    "from util.metric import Metric\n",
    "from dataloaders.cityscape import Cityscape\n",
    "from torchvision.transforms import Compose\n",
    "from torchvision import transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "USE_GPU = True\n",
    "\n",
    "dtype = torch.float32 # we will be using float throughout this tutorial\n",
    "\n",
    "if USE_GPU and torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "    #set the device to gpu0\n",
    "    torch.cuda.set_device(device=0)\n",
    "else:\n",
    "    device = torch.device('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = (417, 417)\n",
    "batch_size=1\n",
    "steps=30000\n",
    "lambda_PAR=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Get the dataset functions from here\n",
    "# dataset_name=\"VOC\"\n",
    "# if dataset_name == 'VOC':\n",
    "#     gen_dataset = voc_fewshot\n",
    "#     data_dir='../../data/Pascal/VOCdevkit/VOC2012/'\n",
    "#     data_split='trainaug'\n",
    "# elif dataset_name == 'COCO':\n",
    "#     gen_dataset = coco_fewshot\n",
    "#     data_dir='../../data/COCO/'\n",
    "#     data_split='train'\n",
    "# # else: \n",
    "# #     dataset=cityscape_fewshot\n",
    "\n",
    "# labels = CLASS_LABELS[dataset_name][0]\n",
    "# transforms = Compose([Resize(size=input_size),\n",
    "#                         RandomMirror()])\n",
    "# dataset = gen_dataset(\n",
    "#     base_dir=data_dir,\n",
    "#     split=data_split,\n",
    "#     transforms=transforms,\n",
    "#     to_tensor=ToTensorNormalize(),\n",
    "#     labels=labels,\n",
    "#     max_iters=batch_size*steps,\n",
    "#     n_ways=1,\n",
    "#     n_shots=1,\n",
    "#     n_queries=1\n",
    "# )\n",
    "\n",
    "# train_loader = DataLoader(\n",
    "#     dataset,\n",
    "#     batch_size=batch_size,\n",
    "#     shuffle=True,\n",
    "#     num_workers=1,\n",
    "#     pin_memory=True,\n",
    "#     drop_last=True\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "select_set=0\n",
    "dataset_name=\"Cityscape\"\n",
    "if dataset_name == 'Cityscape':\n",
    "    cityscapesPath = '../../data/Cityscape'\n",
    "    dataset_size = 30000\n",
    " \n",
    "\n",
    "    flip_transform = transforms.RandomHorizontalFlip(p=0.25)\n",
    "    img_transforms = Compose([transforms.ToTensor(),\n",
    "                          transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225)),\n",
    "                          transforms.Resize(size=input_size),])\n",
    "                          #flip_transform])\n",
    "    mask_transforms = Compose([transforms.ToTensor(),\n",
    "                          transforms.Resize(size=input_size),])\n",
    "                          #flip_transform])\n",
    "\n",
    "    dataset = Cityscape(cityscapesPath, dataset_size, labels=CLASS_LABELS[dataset_name][select_set], split='train',img_transforms=img_transforms, mask_transforms=mask_transforms,\n",
    "                       n_ways=1, n_shots=1, n_queries=1)\n",
    "    labels = CLASS_LABELS[dataset_name][select_set]\n",
    "    labels_val = CLASS_LABELS[dataset_name][\"all\"]-labels\n",
    "    \n",
    "else:\n",
    "    if dataset_name == 'VOC':\n",
    "        gen_dataset = voc_fewshot\n",
    "        data_dir='../../data/Pascal/VOCdevkit/VOC2012/'\n",
    "        data_split='trainaug'\n",
    "    elif dataset_name == 'COCO':\n",
    "        gen_dataset = coco_fewshot\n",
    "        data_dir='../../data/COCO/'\n",
    "        data_split='train'\n",
    "# else:\n",
    "#     dataset=cityscape_fewshot\n",
    "\n",
    "    labels = CLASS_LABELS[dataset_name][select_set]\n",
    "    labels_val = CLASS_LABELS[dataset_name][\"all\"]-labels\n",
    "    transforms = Compose([Resize(size=input_size),\n",
    "                            RandomMirror()])\n",
    "\n",
    "    transforms = Compose([Resize(size=input_size)])\n",
    "    dataset = gen_dataset(\n",
    "        base_dir=data_dir,\n",
    "        split=data_split,\n",
    "        transforms=transforms,\n",
    "        to_tensor=ToTensorNormalize(),\n",
    "        labels=labels,\n",
    "        max_iters=1 *steps,\n",
    "        n_ways=1,\n",
    "        n_shots=1,\n",
    "        n_queries=1\n",
    "    )\n",
    "\n",
    "#     dataset=cityscape_fewshot\n",
    "\n",
    "# labels = CLASS_LABELS[dataset_name][select_set]\n",
    "# labels_val = CLASS_LABELS[dataset_name][\"all\"]-labels\n",
    "# transforms = Compose([Resize(size=input_size),\n",
    "#                         RandomMirror()])\n",
    "# dataset = gen_dataset(\n",
    "#     base_dir=data_dir,\n",
    "#     split=data_split,\n",
    "#     transforms=transforms,\n",
    "#     to_tensor=ToTensorNormalize(),\n",
    "#     labels=labels,\n",
    "#     max_iters=setup['batch_size'] *steps,\n",
    "#     n_ways=setup['ways'],\n",
    "#     n_shots=setup['shots'],\n",
    "#     n_queries=setup['num_queries']\n",
    "# )\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    dataset,\n",
    "    batch_size=1,\n",
    "    shuffle=True,\n",
    "    num_workers=1,\n",
    "    pin_memory=True,\n",
    "    drop_last=True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path= \"./misc/fewshotv3_temp.pth\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30000\n"
     ]
    }
   ],
   "source": [
    "print(len(train_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_every=100\n",
    "save_every=1000\n",
    "def train_model(model, optimizer, scheduler ,epochs=1):\n",
    "\n",
    "    train_loss={}\n",
    "    #lossfn=nn.CosineEmbeddingLoss(margin=0,reduction=\"mean\")\n",
    "    lossfn=nn.MSELoss()\n",
    "    model = model.to(device=device)  # move the model parameters to CPU/GPU\n",
    "    train_loss[\"loss_query\"]=0\n",
    "    train_loss[\"loss_PAR\"]=0\n",
    "    for e in range(epochs):\n",
    "        lossq=0\n",
    "        losspar=0\n",
    "        for t,samples in enumerate(train_loader):\n",
    "            \n",
    "           #get a list of support images ( Sx W x H x W)\n",
    "            support_img=[[shot.cuda() for shot in way]\n",
    "                          for way in samples['support_images']]\n",
    "            fg_mask=[[shot[f'fg_mask'].float().cuda() for shot in way]\n",
    "                           for way in samples['support_mask']]\n",
    "            bg_mask=[[shot[f'bg_mask'].float().cuda() for shot in way]\n",
    "                           for way in samples['support_mask']]\n",
    "            query_img= [query.cuda()\n",
    "                        for query in samples['query_images']]\n",
    "\n",
    "            query_gt = torch.cat([queryGT.long().cuda()\n",
    "                        for queryGT in samples['query_labels']], dim=0)\n",
    "            \n",
    "\n",
    "            # Zero out all of the gradients for the variables which the optimizer\n",
    "            # will update.\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            #get the predicted query and the Prototype alignment loss\n",
    "            pred_query,lossPAR= model(support_img,fg_mask,bg_mask,query_img)\n",
    "\n",
    "            lossQ= criterion(pred_query,query_gt)\n",
    "\n",
    "            loss = lossQ+ lossPAR * lambda_PAR\n",
    "            # This is the backwards pass: compute the gradient of the loss with\n",
    "            # respect to each  parameter of the model.\n",
    "            loss.backward()\n",
    "\n",
    "            # Actually update the parameters of the model using the gradients\n",
    "            # computed by the backwards pass.\n",
    "            optimizer.step()\n",
    "\n",
    "            scheduler.step()\n",
    "\n",
    "            lossq+=lossQ.detach().cpu().numpy()\n",
    "            losspar+=lossPAR\n",
    "\n",
    "            if (t+1) % print_every == 0:\n",
    "                print('Epoch %d, Iteration %d, loss = %.8f' % (e, t+1, (lossq/(t+1))))\n",
    "                print('Epoch %d, Iteration %d, loss = %.8f' % (e, t+1, (losspar/(t+1))))\n",
    "            if (t+1) % save_every == 0:\n",
    "                torch.save(model.state_dict(),model_path)   \n",
    "\n",
    "        train_loss[\"loss_query\"]=lossq/len(train_loader)\n",
    "        train_loss[\"loss_PAR\"]=losspar/len(train_loader)\n",
    "\n",
    "    return (train_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /home/prasad/.cache/torch/hub/pytorch_vision_v0.10.0\n"
     ]
    }
   ],
   "source": [
    "#cfg True means align is on\n",
    "learning_rate=1e-3\n",
    "milestones= [steps//3,steps//2,steps]\n",
    "model = FewShotSegV3(cfg={'align': True},distfunc=\"euclidean\")\n",
    "optimizer = torch.optim.SGD(model.parameters(),lr=learning_rate,momentum=0.9,weight_decay=0.00005)\n",
    "scheduler = MultiStepLR(optimizer, milestones=milestones, gamma=0.1)\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=255)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/prasad/anaconda3/envs/aec/lib/python3.9/site-packages/torchvision/transforms/functional.py:423: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/home/prasad/Personal/ECE285/Project/Few-Shot-Segmentation-ECE-285/panet/PANet/experiments/train_customv3.ipynb Cell 9'\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2B137.110.119.158/home/prasad/Personal/ECE285/Project/Few-Shot-Segmentation-ECE-285/panet/PANet/experiments/train_customv3.ipynb#ch0000008vscode-remote?line=0'>1</a>\u001b[0m train_loss\u001b[39m=\u001b[39mtrain_model(model, optimizer, scheduler ,epochs\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m)\n",
      "\u001b[1;32m/home/prasad/Personal/ECE285/Project/Few-Shot-Segmentation-ECE-285/panet/PANet/experiments/train_customv3.ipynb Cell 7'\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(model, optimizer, scheduler, epochs)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B137.110.119.158/home/prasad/Personal/ECE285/Project/Few-Shot-Segmentation-ECE-285/panet/PANet/experiments/train_customv3.ipynb#ch0000006vscode-remote?line=38'>39</a>\u001b[0m loss \u001b[39m=\u001b[39m lossQ\u001b[39m+\u001b[39m lossPAR \u001b[39m*\u001b[39m lambda_PAR\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B137.110.119.158/home/prasad/Personal/ECE285/Project/Few-Shot-Segmentation-ECE-285/panet/PANet/experiments/train_customv3.ipynb#ch0000006vscode-remote?line=39'>40</a>\u001b[0m \u001b[39m# This is the backwards pass: compute the gradient of the loss with\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B137.110.119.158/home/prasad/Personal/ECE285/Project/Few-Shot-Segmentation-ECE-285/panet/PANet/experiments/train_customv3.ipynb#ch0000006vscode-remote?line=40'>41</a>\u001b[0m \u001b[39m# respect to each  parameter of the model.\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2B137.110.119.158/home/prasad/Personal/ECE285/Project/Few-Shot-Segmentation-ECE-285/panet/PANet/experiments/train_customv3.ipynb#ch0000006vscode-remote?line=41'>42</a>\u001b[0m loss\u001b[39m.\u001b[39;49mbackward()\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B137.110.119.158/home/prasad/Personal/ECE285/Project/Few-Shot-Segmentation-ECE-285/panet/PANet/experiments/train_customv3.ipynb#ch0000006vscode-remote?line=43'>44</a>\u001b[0m \u001b[39m# Actually update the parameters of the model using the gradients\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B137.110.119.158/home/prasad/Personal/ECE285/Project/Few-Shot-Segmentation-ECE-285/panet/PANet/experiments/train_customv3.ipynb#ch0000006vscode-remote?line=44'>45</a>\u001b[0m \u001b[39m# computed by the backwards pass.\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B137.110.119.158/home/prasad/Personal/ECE285/Project/Few-Shot-Segmentation-ECE-285/panet/PANet/experiments/train_customv3.ipynb#ch0000006vscode-remote?line=45'>46</a>\u001b[0m optimizer\u001b[39m.\u001b[39mstep()\n",
      "File \u001b[0;32m~/anaconda3/envs/aec/lib/python3.9/site-packages/torch/_tensor.py:363\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    <a href='file:///home/prasad/anaconda3/envs/aec/lib/python3.9/site-packages/torch/_tensor.py?line=353'>354</a>\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_unary(\u001b[39mself\u001b[39m):\n\u001b[1;32m    <a href='file:///home/prasad/anaconda3/envs/aec/lib/python3.9/site-packages/torch/_tensor.py?line=354'>355</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    <a href='file:///home/prasad/anaconda3/envs/aec/lib/python3.9/site-packages/torch/_tensor.py?line=355'>356</a>\u001b[0m         Tensor\u001b[39m.\u001b[39mbackward,\n\u001b[1;32m    <a href='file:///home/prasad/anaconda3/envs/aec/lib/python3.9/site-packages/torch/_tensor.py?line=356'>357</a>\u001b[0m         (\u001b[39mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    <a href='file:///home/prasad/anaconda3/envs/aec/lib/python3.9/site-packages/torch/_tensor.py?line=360'>361</a>\u001b[0m         create_graph\u001b[39m=\u001b[39mcreate_graph,\n\u001b[1;32m    <a href='file:///home/prasad/anaconda3/envs/aec/lib/python3.9/site-packages/torch/_tensor.py?line=361'>362</a>\u001b[0m         inputs\u001b[39m=\u001b[39minputs)\n\u001b[0;32m--> <a href='file:///home/prasad/anaconda3/envs/aec/lib/python3.9/site-packages/torch/_tensor.py?line=362'>363</a>\u001b[0m torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mbackward(\u001b[39mself\u001b[39;49m, gradient, retain_graph, create_graph, inputs\u001b[39m=\u001b[39;49minputs)\n",
      "File \u001b[0;32m~/anaconda3/envs/aec/lib/python3.9/site-packages/torch/autograd/__init__.py:173\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    <a href='file:///home/prasad/anaconda3/envs/aec/lib/python3.9/site-packages/torch/autograd/__init__.py?line=167'>168</a>\u001b[0m     retain_graph \u001b[39m=\u001b[39m create_graph\n\u001b[1;32m    <a href='file:///home/prasad/anaconda3/envs/aec/lib/python3.9/site-packages/torch/autograd/__init__.py?line=169'>170</a>\u001b[0m \u001b[39m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[1;32m    <a href='file:///home/prasad/anaconda3/envs/aec/lib/python3.9/site-packages/torch/autograd/__init__.py?line=170'>171</a>\u001b[0m \u001b[39m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    <a href='file:///home/prasad/anaconda3/envs/aec/lib/python3.9/site-packages/torch/autograd/__init__.py?line=171'>172</a>\u001b[0m \u001b[39m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> <a href='file:///home/prasad/anaconda3/envs/aec/lib/python3.9/site-packages/torch/autograd/__init__.py?line=172'>173</a>\u001b[0m Variable\u001b[39m.\u001b[39;49m_execution_engine\u001b[39m.\u001b[39;49mrun_backward(  \u001b[39m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    <a href='file:///home/prasad/anaconda3/envs/aec/lib/python3.9/site-packages/torch/autograd/__init__.py?line=173'>174</a>\u001b[0m     tensors, grad_tensors_, retain_graph, create_graph, inputs,\n\u001b[1;32m    <a href='file:///home/prasad/anaconda3/envs/aec/lib/python3.9/site-packages/torch/autograd/__init__.py?line=174'>175</a>\u001b[0m     allow_unreachable\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, accumulate_grad\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train_loss=train_model(model, optimizer, scheduler ,epochs=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: \"https://github.com/pytorch/vision/archive/v0.10.0.zip\" to /Users/udayreddy/.cache/torch/hub/v0.10.0.zip\n",
      "Downloading: \"https://download.pytorch.org/models/deeplabv3_resnet101_coco-586e9e4e.pth\" to /Users/udayreddy/.cache/torch/hub/checkpoints/deeplabv3_resnet101_coco-586e9e4e.pth\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "96973bd9fd96494aad71786c3c3ae93c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0.00/233M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: './test_models/model/fs_0906214253.pth'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/Users/udayreddy/Desktop/ECE 285/Project/Few-Shot-Segmentation-ECE-285/panet/src/train_customv3.ipynb Cell 11'\u001b[0m in \u001b[0;36m<cell line: 5>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/udayreddy/Desktop/ECE%20285/Project/Few-Shot-Segmentation-ECE-285/panet/src/train_customv3.ipynb#ch0000010?line=1'>2</a>\u001b[0m model_eval\u001b[39m=\u001b[39mFewShotSegV3(cfg\u001b[39m=\u001b[39m{\u001b[39m\"\u001b[39m\u001b[39malign\u001b[39m\u001b[39m\"\u001b[39m:\u001b[39mTrue\u001b[39;00m},distfunc\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mcosine\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/udayreddy/Desktop/ECE%20285/Project/Few-Shot-Segmentation-ECE-285/panet/src/train_customv3.ipynb#ch0000010?line=2'>3</a>\u001b[0m \u001b[39m#model_eval=FewShotSeg(cfg={\"align\":True})\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/udayreddy/Desktop/ECE%20285/Project/Few-Shot-Segmentation-ECE-285/panet/src/train_customv3.ipynb#ch0000010?line=3'>4</a>\u001b[0m \u001b[39m#model_eval=FewShotSegV3Inet(cfg={\"align\":True},distfunc=\"cosine\")\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/udayreddy/Desktop/ECE%20285/Project/Few-Shot-Segmentation-ECE-285/panet/src/train_customv3.ipynb#ch0000010?line=4'>5</a>\u001b[0m model_eval\u001b[39m.\u001b[39mload_state_dict(torch\u001b[39m.\u001b[39;49mload(model_path))\n",
      "File \u001b[0;32m~/miniforge3/envs/torch-nightly/lib/python3.8/site-packages/torch/serialization.py:699\u001b[0m, in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, **pickle_load_args)\u001b[0m\n\u001b[1;32m    696\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39m'\u001b[39m\u001b[39mencoding\u001b[39m\u001b[39m'\u001b[39m \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m pickle_load_args\u001b[39m.\u001b[39mkeys():\n\u001b[1;32m    697\u001b[0m     pickle_load_args[\u001b[39m'\u001b[39m\u001b[39mencoding\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mutf-8\u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m--> 699\u001b[0m \u001b[39mwith\u001b[39;00m _open_file_like(f, \u001b[39m'\u001b[39;49m\u001b[39mrb\u001b[39;49m\u001b[39m'\u001b[39;49m) \u001b[39mas\u001b[39;00m opened_file:\n\u001b[1;32m    700\u001b[0m     \u001b[39mif\u001b[39;00m _is_zipfile(opened_file):\n\u001b[1;32m    701\u001b[0m         \u001b[39m# The zipfile reader is going to advance the current file position.\u001b[39;00m\n\u001b[1;32m    702\u001b[0m         \u001b[39m# If we want to actually tail call to torch.jit.load, we need to\u001b[39;00m\n\u001b[1;32m    703\u001b[0m         \u001b[39m# reset back to the original position.\u001b[39;00m\n\u001b[1;32m    704\u001b[0m         orig_position \u001b[39m=\u001b[39m opened_file\u001b[39m.\u001b[39mtell()\n",
      "File \u001b[0;32m~/miniforge3/envs/torch-nightly/lib/python3.8/site-packages/torch/serialization.py:231\u001b[0m, in \u001b[0;36m_open_file_like\u001b[0;34m(name_or_buffer, mode)\u001b[0m\n\u001b[1;32m    229\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_open_file_like\u001b[39m(name_or_buffer, mode):\n\u001b[1;32m    230\u001b[0m     \u001b[39mif\u001b[39;00m _is_path(name_or_buffer):\n\u001b[0;32m--> 231\u001b[0m         \u001b[39mreturn\u001b[39;00m _open_file(name_or_buffer, mode)\n\u001b[1;32m    232\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    233\u001b[0m         \u001b[39mif\u001b[39;00m \u001b[39m'\u001b[39m\u001b[39mw\u001b[39m\u001b[39m'\u001b[39m \u001b[39min\u001b[39;00m mode:\n",
      "File \u001b[0;32m~/miniforge3/envs/torch-nightly/lib/python3.8/site-packages/torch/serialization.py:212\u001b[0m, in \u001b[0;36m_open_file.__init__\u001b[0;34m(self, name, mode)\u001b[0m\n\u001b[1;32m    211\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\u001b[39mself\u001b[39m, name, mode):\n\u001b[0;32m--> 212\u001b[0m     \u001b[39msuper\u001b[39m(_open_file, \u001b[39mself\u001b[39m)\u001b[39m.\u001b[39m\u001b[39m__init__\u001b[39m(\u001b[39mopen\u001b[39;49m(name, mode))\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: './test_models/model/fs_0906214253.pth'"
     ]
    }
   ],
   "source": [
    "model_path= \"./test_models/model/fs_0906214253.pth\"\n",
    "model_eval=FewShotSegV3(cfg={\"align\":True},distfunc=\"cosine\")\n",
    "#model_eval=FewShotSeg(cfg={\"align\":True})\n",
    "#model_eval=FewShotSegV3Inet(cfg={\"align\":True},distfunc=\"cosine\")\n",
    "model_eval.load_state_dict(torch.load(model_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model_eval' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/Users/udayreddy/Desktop/ECE 285/Project/Few-Shot-Segmentation-ECE-285/panet/src/train_customv3.ipynb Cell 12'\u001b[0m in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/udayreddy/Desktop/ECE%20285/Project/Few-Shot-Segmentation-ECE-285/panet/src/train_customv3.ipynb#ch0000011?line=12'>13</a>\u001b[0m query_img\u001b[39m=\u001b[39m [query\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/udayreddy/Desktop/ECE%20285/Project/Few-Shot-Segmentation-ECE-285/panet/src/train_customv3.ipynb#ch0000011?line=13'>14</a>\u001b[0m             \u001b[39mfor\u001b[39;00m query \u001b[39min\u001b[39;00m sample[\u001b[39m'\u001b[39m\u001b[39mquery_images\u001b[39m\u001b[39m'\u001b[39m]]\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/udayreddy/Desktop/ECE%20285/Project/Few-Shot-Segmentation-ECE-285/panet/src/train_customv3.ipynb#ch0000011?line=14'>15</a>\u001b[0m query_gt \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mcat([queryGT\u001b[39m.\u001b[39mlong()\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/udayreddy/Desktop/ECE%20285/Project/Few-Shot-Segmentation-ECE-285/panet/src/train_customv3.ipynb#ch0000011?line=15'>16</a>\u001b[0m                         \u001b[39mfor\u001b[39;00m queryGT \u001b[39min\u001b[39;00m sample[\u001b[39m'\u001b[39m\u001b[39mquery_labels\u001b[39m\u001b[39m'\u001b[39m]], dim\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m)\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/udayreddy/Desktop/ECE%20285/Project/Few-Shot-Segmentation-ECE-285/panet/src/train_customv3.ipynb#ch0000011?line=17'>18</a>\u001b[0m model\u001b[39m=\u001b[39mmodel_eval\u001b[39m.\u001b[39mto(device\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mcpu\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/udayreddy/Desktop/ECE%20285/Project/Few-Shot-Segmentation-ECE-285/panet/src/train_customv3.ipynb#ch0000011?line=18'>19</a>\u001b[0m model\u001b[39m.\u001b[39meval()\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/udayreddy/Desktop/ECE%20285/Project/Few-Shot-Segmentation-ECE-285/panet/src/train_customv3.ipynb#ch0000011?line=19'>20</a>\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mno_grad():\n",
      "\u001b[0;31mNameError\u001b[0m: name 'model_eval' is not defined"
     ]
    }
   ],
   "source": [
    "#Prediction \n",
    "idx=0\n",
    "\n",
    "for idx in range(3):\n",
    "    iterator=iter(train_loader)\n",
    "    sample=iterator.next()\n",
    "    support_img=[[shot for shot in way]\n",
    "                    for way in sample['support_images']]\n",
    "    fg_mask=[[shot[f'fg_mask'].float() for shot in way]\n",
    "                    for way in sample['support_mask']]\n",
    "    bg_mask=[[shot[f'bg_mask'].float() for shot in way]\n",
    "                    for way in sample['support_mask']]\n",
    "    query_img= [query\n",
    "                for query in sample['query_images']]\n",
    "    query_gt = torch.cat([queryGT.long()\n",
    "                            for queryGT in sample['query_labels']], dim=0)\n",
    "\n",
    "    # model=model_eval.to(device=\"cpu\")\n",
    "    # model.eval()\n",
    "    # with torch.no_grad():\n",
    "    #     pred_query,lossPAR= model(support_img,fg_mask,bg_mask,query_img)\n",
    "\n",
    "    # pred=np.array(pred_query.argmax(dim=1)[0].cpu())\n",
    "    gt_query=query_gt[0,:,:].cpu().numpy()\n",
    "\n",
    "    test=query_img[0][0].cpu().numpy()\n",
    "    test=np.transpose(test,(1,2,0))\n",
    "    testn=cv2.normalize(test, None, alpha=0, beta=1, norm_type=cv2.NORM_MINMAX, dtype=cv2.CV_32F)\n",
    "\n",
    "    overlay=np.zeros_like(test)\n",
    "    gt_overlay=np.zeros_like(test)\n",
    "\n",
    "    # overlay[:,:,0]=1.0\n",
    "    # overlay[:,:,0]=overlay[:,:,0]*pred\n",
    "\n",
    "    gt_overlay[:,:,0]=1.0\n",
    "    gt_overlay[:,:,0]=gt_overlay[:,:,0]*gt_query\n",
    "\n",
    "    test_gt=cv2.addWeighted(testn,1,gt_overlay,0.9,0)\n",
    "    test_pred=cv2.addWeighted(testn,1,overlay,0.9,0)\n",
    "\n",
    "    plt.figure(idx,figsize=(15,15))\n",
    "    ax=plt.subplot(1,3,1)\n",
    "    ax.imshow(testn)\n",
    "    ax.title.set_text(\"Test Image\")\n",
    "    ax=plt.subplot(1,3,2)\n",
    "    ax.imshow(test_gt)\n",
    "    ax.title.set_text(\"Ground Truth Segmented\")\n",
    "    ax=plt.subplot(1,3,3)\n",
    "    ax.imshow(test_pred)\n",
    "    ax.title.set_text(\"Predicted Segmentation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m/home/prasad/Personal/ECE285/Project/Few-Shot-Segmentation-ECE-285/panet/PANet/experiments/train_customv3.ipynb Cell 14'\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B137.110.119.158/home/prasad/Personal/ECE285/Project/Few-Shot-Segmentation-ECE-285/panet/PANet/experiments/train_customv3.ipynb#ch0000013vscode-remote?line=0'>1</a>\u001b[0m max_label \u001b[39m=\u001b[39m \u001b[39m20\u001b[39m\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B137.110.119.158/home/prasad/Personal/ECE285/Project/Few-Shot-Segmentation-ECE-285/panet/PANet/experiments/train_customv3.ipynb#ch0000013vscode-remote?line=1'>2</a>\u001b[0m metric \u001b[39m=\u001b[39m Metric(max_label\u001b[39m=\u001b[39mmax_label, n_runs\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2B137.110.119.158/home/prasad/Personal/ECE285/Project/Few-Shot-Segmentation-ECE-285/panet/PANet/experiments/train_customv3.ipynb#ch0000013vscode-remote?line=2'>3</a>\u001b[0m classIoU, meanIoU \u001b[39m=\u001b[39m metric\u001b[39m.\u001b[39;49mget_mIoU(labels\u001b[39m=\u001b[39;49m\u001b[39msorted\u001b[39;49m(labels), n_run\u001b[39m=\u001b[39;49m\u001b[39m30000\u001b[39;49m)\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B137.110.119.158/home/prasad/Personal/ECE285/Project/Few-Shot-Segmentation-ECE-285/panet/PANet/experiments/train_customv3.ipynb#ch0000013vscode-remote?line=3'>4</a>\u001b[0m classIoU_binary, meanIoU_binary \u001b[39m=\u001b[39m metric\u001b[39m.\u001b[39mget_mIoU_binary(n_run\u001b[39m=\u001b[39m\u001b[39m30000\u001b[39m)\n",
      "File \u001b[0;32m~/Personal/ECE285/Project/Few-Shot-Segmentation-ECE-285/panet/PANet/experiments/util/metric.py:99\u001b[0m, in \u001b[0;36mMetric.get_mIoU\u001b[0;34m(self, labels, n_run)\u001b[0m\n\u001b[1;32m     <a href='file:///home/prasad/Personal/ECE285/Project/Few-Shot-Segmentation-ECE-285/panet/PANet/experiments/util/metric.py?line=95'>96</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m (mIoU_class\u001b[39m.\u001b[39mmean(axis\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m), mIoU_class\u001b[39m.\u001b[39mstd(axis\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m),\n\u001b[1;32m     <a href='file:///home/prasad/Personal/ECE285/Project/Few-Shot-Segmentation-ECE-285/panet/PANet/experiments/util/metric.py?line=96'>97</a>\u001b[0m             mIoU\u001b[39m.\u001b[39mmean(axis\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m), mIoU\u001b[39m.\u001b[39mstd(axis\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m))\n\u001b[1;32m     <a href='file:///home/prasad/Personal/ECE285/Project/Few-Shot-Segmentation-ECE-285/panet/PANet/experiments/util/metric.py?line=97'>98</a>\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m---> <a href='file:///home/prasad/Personal/ECE285/Project/Few-Shot-Segmentation-ECE-285/panet/PANet/experiments/util/metric.py?line=98'>99</a>\u001b[0m     tp_sum \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mnansum(np\u001b[39m.\u001b[39mvstack(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtp_lst[n_run]), axis\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m)\u001b[39m.\u001b[39mtake(labels)\n\u001b[1;32m    <a href='file:///home/prasad/Personal/ECE285/Project/Few-Shot-Segmentation-ECE-285/panet/PANet/experiments/util/metric.py?line=99'>100</a>\u001b[0m     fp_sum \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mnansum(np\u001b[39m.\u001b[39mvstack(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfp_lst[n_run]), axis\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m)\u001b[39m.\u001b[39mtake(labels)\n\u001b[1;32m    <a href='file:///home/prasad/Personal/ECE285/Project/Few-Shot-Segmentation-ECE-285/panet/PANet/experiments/util/metric.py?line=100'>101</a>\u001b[0m     fn_sum \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mnansum(np\u001b[39m.\u001b[39mvstack(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfn_lst[n_run]), axis\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m)\u001b[39m.\u001b[39mtake(labels)\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "max_label = 20\n",
    "metric = Metric(max_label=max_label, n_runs=1)\n",
    "classIoU, meanIoU = metric.get_mIoU(labels=sorted(labels), n_run=30000)\n",
    "classIoU_binary, meanIoU_binary = metric.get_mIoU_binary(n_run=30000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input dims': [417, 417], 'steps': 30000, 'ways': 1, 'shots': 1, 'num_queries': 1, 'batch_size': 1, 'learning_rate': 0.001, 'distfunc': 'cosine', 'backbone': 'dv3', 'dataset': 'VOC', 'desctiption': 'Experiment1: baseline with dv3 encoder', 'optimizer': 'SGD (\\nParameter Group 0\\n    dampening: 0\\n    initial_lr: 0.001\\n    lr: 0.001\\n    maximize: False\\n    momentum: 0.9\\n    nesterov: False\\n    weight_decay: 5e-05\\n)', 'epochs': 1}\n"
     ]
    }
   ],
   "source": [
    "import json \n",
    "\n",
    "dir= \"./test_models/info/fs_0906182026.json\"\n",
    "openfile=open(dir, \"r\")\n",
    "setup= json.load(openfile)\n",
    "\n",
    "print(setup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('torch-nightly')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "549bbba919a6dc2076479c442657ade3b2767403873ad6434eaf2210fb7b8f6e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
